{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing necessary libraries and loading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "import google.generativeai as genai\n",
    "# from langchain.llms import GooglePalm\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load environment variables and configure Google Generative AI\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])\n",
    "\n",
    "# Note: max_output_tokens or prompting to balance the length of the generated output.\n",
    "llm = GoogleGenerativeAI(model=\"models/text-bison-001\", temperature=0.1)\n",
    "# llm = GooglePalm(model_name=\"models/text-bison-001\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loading the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in the loaded vector store: 47\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Load the embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Now try to load the vector store\n",
    "loaded_vectorstore = Chroma(persist_directory=\"./chroma_db\", embedding_function=embeddings)\n",
    "\n",
    "print(f\"Number of documents in the loaded vector store: {loaded_vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"You are an AI assistant tasked with writing a comprehensive document in Markdown format based on a provided table of contents. \n",
    "Use the following pieces of context to write detailed sections for the document.\n",
    "If you don't have enough information, state that more research is needed on that topic.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Section to write:\n",
    "{question}\n",
    "\n",
    "AI Assistant: Write a detailed section for the given header in Markdown format. \n",
    "Provide comprehensive and informative content directly related to the section title.\n",
    "Do not create additional headers or a table of contents.\n",
    "Ensure the content is well-structured, relevant to the topic, and flows logically.\n",
    "Use the appropriate number of '#' symbols for the header level as indicated in the section title.\n",
    "Do not mention or include any links to images or any other form of data other than text.\n",
    "Focus solely on providing textual content relevant to the section.\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Update the qa chain with the new prompt\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=loaded_vectorstore.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT}\n",
    ")\n",
    "\n",
    "def notegen(query):\n",
    "    result = qa.invoke({\"query\": query})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##2. Core Architectural Components\n",
      "Saved!\n",
      "###2.1 Transformer Architecture\n",
      "Saved!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# def generate_document(toc):\n",
    "    \n",
    "#     # Tokenize the input\n",
    "#     sections = [section.strip() for section in toc.split(',')]\n",
    "    \n",
    "#     # Open the output file\n",
    "#     with open('output.md', 'w', encoding='utf-8') as f:\n",
    "#         # Iterate through each section\n",
    "#         for section in sections:\n",
    "            \n",
    "#             # Generate content for the section\n",
    "#             result = notegen(section)\n",
    "\n",
    "#             print(section)\n",
    "            \n",
    "#             # Write the section header\n",
    "#             f.write(f\"{section}\\n\\n\")\n",
    "            \n",
    "#             # Write the generated content\n",
    "#             f.write(f\"{result['result']}\\n\\n\")\n",
    "    \n",
    "#     print(\"Saved!\")\n",
    "\n",
    "def generate_document(toc):\n",
    "\n",
    "    # Open the output file\n",
    "    with open('output.md', 'a', encoding='utf-8') as f:\n",
    "            \n",
    "        # Generate content for the section\n",
    "        result = notegen(toc)\n",
    "\n",
    "        print(toc)\n",
    "        \n",
    "        # Write the section header\n",
    "        # f.write(f\"{toc}\\n\\n\")\n",
    "        \n",
    "        # Write the generated content\n",
    "        f.write(f\"{result['result']}\\n\\n\")\n",
    "    \n",
    "    print(\"Saved!\")\n",
    "\n",
    "# toc = input(\"Please enter the table of contents in one line, separated by commas: \")\n",
    "\n",
    "# toc = \"\"\"# Understanding Large Language Model Architectures, ##1. Fundamentals of LLMs, ###1.1 Definition and Key Concepts, ###1.2 Historical Development of LLMs, ##2. Core Architectural Components, ###2.1 Transformer Architecture, ###2.2 Attention Mechanism, ###2.3 Self-Attention Mechanism, ##3. Training and Fine-Tuning LLMs, ###3.1 Data Preprocessing, ###3.2 Training Process, ###3.3 Fine-Tuning Process\"\"\"\n",
    "\n",
    "toc1 = \"\"\"# Understanding Large Language Model Architectures\"\"\"\n",
    "toc2 = \"\"\"##1. Fundamentals of LLMs\"\"\"\n",
    "toc3 = \"\"\"##2. Core Architectural Components\"\"\"\n",
    "toc4 = \"\"\"###2.1 Transformer Architecture\"\"\"\n",
    "\n",
    "# Call the function\n",
    "# generate_document(toc1)\n",
    "# generate_document(toc2)\n",
    "generate_document(toc3)\n",
    "generate_document(toc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##2. Core Architectural Components, ###2.1 Transformer Architecture"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spambots",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
